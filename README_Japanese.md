# 私のお気に入りの統計量：ヘフディングのD
![イラストレーション](https://raw.githubusercontent.com/Dicklesworthstone/hoeffdings_d_explainer/main/hoeffd_illustration.webp)

想像してみてください。二つの数字の列を比較して、それらがどの程度関連しているか、または依存しているかを測定したいとします。これは実に一般的な設定です。二つの列は時間系列を表しているかもしれません。つまり、3つの列とたくさんの行を持つ表があります。最初の列はタイムスタンプ（例えば、1時間ごと）、次に各列が一つずつあります。最初の列は、その間隔での株式の平均価格であるかもしれませんし、二番目の列は、その間隔で取引された株式の量かもしれません。または、ある株の価格変動率を別の株のそれと比較することもできます。もちろん、時間系列である必要は全くありません。2つの列だけ（つまり、タイムスタンプの列がまったくない）でも構いません。一つ目の列は30歳以上のアメリカ人の身長（インチ単位）、二つ目の列はその人の体重（ポンド単位）かもしれません。または、よりタイムリーな例を使うと、各列が特定のLLMからのある英語の文の埋め込みベクトルを表しているかもしれません。最初の列は「I love my 3 sons」という文字列に対するMixtral 8x7Bモデルからの埋め込みベクトルであり、もう一つは同じモデルからの「I cherish my 5 daughters」という文字列に対する埋め込みベクトルかもしれません。

これらのケースでは、比較したい二つのデータ列があります。問題は、最も一般的な設定では、関係の性質が事前に何であるか、またはそもそも関係が存在するのかどうか、まったくわからないかもしれないことです。もし二つの列が、異なる公正なサイコロのロールの記録のように、完全に独立しているとしたらどうでしょう？データに少し問題があって、いくつかの極端な外れ値が含まれているため、各列の平均や分散など、見たいと思う最も一般的な種類の尺度を歪めるかもしれません。「ちょっと待って、これに対する答えはただの相関を見ることじゃないの？」と今思うかもしれません。確かに、それは二つのデータセット間の連関を測定する最も一般的に使用される尺度なので、チェックするには良いアイデアです。

用語を明確にするために、「相関」と一般的に言及されるのは、1800年代にさかのぼるピアソンの相関係数を指します。この相関は、実際にはデータ間の共分散を再スケーリングしたもので、特定の単位に依存しない尺度を提供します。では、共分散とは何でしょうか？直感的には、まず個々の列をそれぞれ見て、その平均値を計算します。次に、その列の個々のデータポイントがその平均からどの程度逸脱しているかを見ます（つまり、その列の平均値から各データポイントを引くだけです）。次に、これらの尺度を各列で比較するために、それらを掛け合わせます。もし列が同様のパターンを示し、最初の列のエントリがその最初の列の平均よりも大きい傾向にある「同時に」二番目の列についてもこれが当てはまる場合、それは列が関連していることを示唆し、これらの尺度の積は高くなります。（「同時に」という言葉を引用符で囲んだのは、列が時間系列である必要はなく、「両方の列のK番目のエントリで」という本当の意味であるからです。時間について話すことで理解しやすくなります）。もし列が本当に互いに何の関係もない場合（例えば、公平なサイコロのロールを単に記録しているケースのように）、この積はほぼゼロに近くなります。なぜなら、一方の列がその平均値を上回るエントリを持つのと同時に、他方の列がその平均値を下回るエントリを持つのと同じくらいありえるからです。

共分散を再スケーリングして次元のない数値を提供するピアソン相関は、素晴らしく解釈しやすい結果を提供します。それは常に-1.0から1.0の間です。0.0であれば、列間に「関係性がない」ことを意味し、1.0であれば完全に相関しており一致して動きます。-1.0であれば、正反対です。一方が上がると、もう一方は同じ量だけ下がります。これはかなり良い尺度のように聞こえますよね？正確には何が問題なのでしょうか？問題は、ピアソン相関を使用するとき、あなたは暗黙のうちに二つの列間に特定の種類の関係、つまり線形関係を探していることです。そして、比較したいと思うかもしれない人生の中の多くのものが、自然にはまったく線形ではないのです。

具体例を使ってものごとをもっと具体的にしましょう。上述した人々の身長と体重の例を使います。そこではおおよそ線形の関係があると期待されます。確かに、非常に背が低くて非常に太っている人もいれば、非常に背が高くて非常に痩せている人もいますが、平均的には、身長をX軸に、体重をY軸に示した散布図を見たとき、サンプル内の各点が一人の人を表しているとして、全体の人口から偏りなく十分な数の人を取れば、このデータに線を当てはめることで大まかなモデルを得ることができると期待されます。このモデルが実際のデータポイントと比較してどの程度正確であるかは、R二乗と呼ばれ、基本的には一方のデータ列の分散が他方のデータ列によってどれだけ説明されているかの割合です。

この場合、ピアソンの相関係数はうまく機能し、計算が比較的簡単で迅速です（例えばExcelで簡単に計算できます）し、理解や解釈もシンプルなので、関連性や依存性の「行く先」となる尺度になっています。しかし、そのような線形でない明確に理解できるデータ列間の関連性がある多くの他の状況を考えることができます。例えば、成人の体重と50ヤードダッシュレースでの最高速度を考えてみてください。非常に痩せている人は速筋脚筋肉が少ないかもしれず、したがって速度が遅いかもしれません。そして、体重が増えるにつれて最初は平均最高速度が増加します。しかし、明らかに非常に肥満の人は走る速度が遅くなるので、ある点から平均最高速度は減少し始め、体重が非常に肥満度が高いレベルになると急激に落ち込みます。要するに、線をうまく当てはめることができないものです。ピアソンの相関係数の問題は、このような状況では信頼できないということです。

ピアソンの相関係数に関するもう一つの欠点は以前に触れた通り、外れ値への敏感さです。データ入力者がランナーに関するデータの数字を一つ抜かしたり、ある人の体重にゼロをいくつか追加したりしたらどうなるでしょう？たとえデータセットが千人分あったとしても、一人が誤って200万ポンドの体重でデータ入力されていた場合、それはあなたの測定値を非常に大きく狂わせる可能性があります。この特定の例は非現実的に聞こえるかもしれませんが、特に大規模なデータセットを扱う場合には、この種の外れ値による悪いデータの問題は実際に非常に一般的です。

外れ値の問題に対処するために、ピアソンの相関係数に小さな修正が提案されています。例えば、スピアマンの順位相関は、本質的にはピアソンの相関係数と同じですが、データポイントをそれぞれのシーケンス内でのランクに置き換えてから行います。たとえば、前述の例で最も重かった人が200万ポンドだった場合、その超高値は1000（そのデータセットに1000人いて、200万ポンドの人がそのセットで最も重いため）に置き換えられます。これにより、ピアソンのものと全体的に似た方法で機能するが、外れ値に強い尺度が得られます。

さらに精緻化された方法として、ケンドールの順位相関係数があります。これもデータポイントをそれぞれのランクに置き換えますが、スピアマンの順位相関がすべてのデータポイントを一緒に見るのに対し、ケンドールの順位相関は個々のデータポイントのペアを取ります。たとえば、各シーケンスから3番目のデータポイントを取り、それらがそれぞれのシーケンス内でのランクにおいて調和しているか（両方のランクが別のペアよりも高いか低い）または矛盾しているか（一方のシーケンスでランクが高いが、もう一方では低い—これらは「一致」と「不一致」のペアと呼ばれます）を比較します。このプロセスを全データポイントに対して繰り返し、基本的には可能なすべてのペアの間の合意と不一致の集計を取るようなものです。ここでの微妙な点は、各シーケンスのK番目のエントリーがそれぞれのシーケンス内で同じランクである場合のタイの扱い方です。ケンドールの順位相関係数は、連合尺度を効果的に「再スケーリング」または正規化できるように、タイの数を考慮に入れます。直感的には、XとYという二つのベクトルがそれぞれ100万の数値観測値を含んでいて、そのうちの5つを除くすべてがXとYの間で

要素ごとに同じである場合と、XとYの長さが10で、5つの異なるエントリーがある場合とでは、非常に異なる状況です。

ですから、ケンドールのタウは外れ値に効果的に対処し、ピアソンの相関とは異なり、系列間に線形関係が存在するとは仮定しません。しかし、二つの系列間に存在するかもしれない特定の種類の関係の存在を信頼性高く検出するという点で、依然として深刻な問題があります。特に複雑な非線形関係を明らかにする場合にはそうです。ケンドールのタウの欠点の一つ、そして多くの他のランクベースの方法の欠点は、サイクル的な関係や複数の変数が非単調な方法で相互作用するような、より複雑な連関パターンを扱う場合に崩壊することです。それはケンドールのタウが、変数が互いに一貫した方向に動く単調な関係に対してのみうまく機能する、ペア間の一致と不一致に焦点を当てているためです。

問題は実際には、関係が単調かどうかということ以上に複雑です。この問題をその全般性で理解する最良の方法は、二つの系列間の関係について何か明らかで明確なことが視覚的に見て取れるような、少し奇妙な散布図を思い浮かべることです。しかし、これまでに話した尺度ではほとんど何も言えないような場合です。例えば、散布図がリングや「X」の形をしているとします。ここで私が話していることの視覚的な感覚を得たい場合、[このページ](https://www.wolfram.com/mathematica/new-in-9/enhanced-probability-and-statistics/use-hoeffdings-d-to-quantify-and-test-non-monotoni.html)には非常に良い例があり、通常見落とされる様々な奇妙な形をヘフディングのDが捉える方法を示しています。それらは、ケンドールのタウやピアソンの相関を使用して測定した場合、ほぼゼロの関連性を持つと見なされるかもしれませんが、視覚的に物事を行う人に、点が垂直軸上でどこにあるべきか（またはその逆）を尋ねた場合、彼らは非常に良いアイデアを持っているかもしれません（ただし、これは「ここのリング構造の上部、または下部のどちらか—しかし、おそらく一方か他方です」といったように表現されるかもしれません）。その種の関係はランダムからは程遠いものですが、基本的な仮定（つまり、関係が一変数の関数を使用して記述でき、あなたが微積分の授業から覚えているかもしれない「垂直線」テストを通過する）を破るため、これらより単純な連関尺度にはほとんど「見えない」ものです。

LLMや言語テキストの文字列間の「意味的類似性」を定量化するために使用される埋め込みベクトルの類似性に立脚してこのトピックにアプローチしている場合、「でも、コサイン類似性についてはどうなの？それがほぼ全てのベクトルデータベースやRAGパイプラインで使用される金の標準じゃないの？」と思うかもしれませんね。良い質問です！コサイン類似性は確かに非常に有用であり、何よりも、数百万、あるいは数十億のベクトルに対して迅速かつ効率的に計算できるという重要な利点があります。直感的には、コサイン類似性は、各シーケンスをN次元空間の点として考えることによって機能します。もし各埋め込みベクトルが2048個の数字で構成されていて、これらのベクトルが2つある場合、それらを2048次元空間の2つの個別の点として考えることができます。次に、これらの様々な次元にわたって、各点が原点に対して作る角度を比較します。もし点が原点に対して類似した角度を持つベクトルを張るなら、それらはある意味で「この高次元空間で同じ一般的な方向を指している」と言え、したがって、互いに「類似している」と言えます。この概念化の問題は、3次元を超えると私たちの空間に対する直感が崩壊し始め、2048次元になると、物事は非常に奇妙で直感に反するものになるということです。例えば、そのような高次元空間での球体や立方体の一般化は、その表面近くにほぼ全ての体積が含まれることになりますが、3Dの球体や立方体では真逆が真実です。

それにもかかわらず、コサイン類似性は、干し草の山の中から針を見つける近似的な場所を見つけるのに非常に便利です。何千もの本からの数百万の文の埋め込みベクトルをベクトルデータベースに持っていて、「古代の最も偉大な数学者は一般にアルキメデスだと考えられている」という文に似た文を見つけたい場合、この非常に具体的な考えと何の関係もない数百万の文の99.999％を迅速に排除するのに非常に効果的です。なぜなら、これらの文のほとんどは、埋め込み空間の非常に異なる場所を指し示す埋め込みベクトルに対応するからです。しかし、ほぼすべての保存されたベクトルをフィルタリングした後、最も類似した20の文が残り、これらをランク付けして最も関連性の高いものを最初に表示したい場合はどうでしょうか？ここでは、コサイン類似性がある種の鈍い道具になり得ること、さまざまな方法で歪められ得ることを私は主張します。しかし、もっと良い例は、ベクトルデータベースに明らかに関連する文が単に存在しない場合かもしれません。そのため、コサイン類似性を介して見つかった上位20の「最も類似した」文が特に関連性が高いとは思えない場合です。この場合、コサイン類似性を介して見つかった上位1％または0.1％の関連ベクトルに適用できる、関連性や依存性の別の尺度を持ちたいかもしれません。これにより、これらのベクトルのランク順序付けが可能になります。

ですから、なぜ私たちがもっと強力で一般的な関連性や依存性の尺度を見つけたいかが理解できるでしょう。その尺度は、二つのデータ列間の可能な関係の性質についての仮定をせず、関係が1対1の関数や単調であることを要求せず、誤った外れ値データポイントを容易に許容できるものです。私の主張は、この目的に対してこれまでに発見された最良の尺度がヘフディングのDであるということです。これは、ポーランドの数学者ワシリー・ヘフディングによって彼の1948年の論文「独立のノンパラメトリックテスト」で初めて紹介されました。この12ページの論文で、ヘフディングは彼がD（依存性のため）と呼ぶものを定義しています。数学的な訓練を受けているなら、彼の元の論文を読みたいかもしれませんが、多くの人にとっては理解しにくく直感的ではないと思われるかもしれません。それでも、それが最もシンプルで直感的な方法で提示されれば理解できないというわけではない、そういった根本的に難しい概念ではないと私は今、試みようと思います。

ケンドールのタウのように、ヘフディングのDの計算も似たような方法から始まります：まず、二つの列の各値を、それぞれの列内でのその値の順位に置き換えます。もし複数の値が完全に等しく「タイ」となる場合は、等しい値の順位の平均を取ります。したがって、ある列の1000データポイント中で特定のデータポイントが順位252を持つようにする4.2という値があるが、実際にはその列に4.2という正確な値を持つポイントが4つある場合、これらのポイントはそれぞれ（252 + 253 + 254 + 255）/ 4 = 253.5の平均順位を受け取ります。次に、各列からの点のペアを見て、「一致」または「不一致」の数を調べます。これも再びケンドールのタウの動作と似ています。しかし、その後のプロセスは異なります：データをランキングし、ペア間の一致と不一致を考慮した後、ヘフディングのDは二つの列間の依存性を定量化する独自のアプローチを導入します。それは、ランクの「共同分布」の観測値と、二つの列が独立している場合に期待されるものとの差に基づく統計量を計算します。

まず、「共同分布」と私たちの二つの列の各々内の個別または「周辺」分布とを比較することについて、一歩後退して説明しましょう。ヘフディングのDの文脈では、「ランクの共同分布」と話すとき、二つの列のランクがデータポイントの全てのペアを通じてどのように結合または関連しているかを指しています。一つの列からのランクをx軸とし、もう一つの列からのランクをy軸とするグラフを想像してみてください。このグラフ上の各点は、それぞれの列からの一組のランクを表しています。これらの点がグラフ上に形成するパターンは、それらの共同分布を反映しています：それは一つの列のランクが他の列のランクとどのように関連しているかを示しています。

一方、「周辺分布」は、単一の列内のランクに関連し、それを単独で考えたものです。先ほどのグラフの一方の軸（x軸またはy軸）だけを見て、もう一方を無視した場合、その軸に沿った点の分布がその列の周辺分布を表します。これは、その列だけ内のランクの広がりや分布について教えてくれますが、他の列からのランクとどのようにペアになるかは考慮しません。

共同分布と周辺分布の違いを理解することは、ヘフディングのDがどのように機能するかを把握する上で重要です。この尺度は、観測された共同分布のランクが、列が独立している場合に期待されるものからどの程度逸脱しているかを評価するものです。独立性の下では、共同分布は単に周辺分布の積になります。つまり、列間でランクがペアになる方法はランダムであり、識別可能なパターンはありません。しかし、列間に依存関係がある場合、観測された共同分布はこの周辺分布の積とは異なり、一方の列のランクが他方の列のランクと系統的に関連していることを示します。ヘフディングのDはこの差を定量化し、列間の依存関係の統計的尺度を提供します。

この目的のために、ヘフディングのDは、ランクペアのすべての可能な「四つ組」を考慮に入れます。つまり、任意の4つの異なるデータポイントについて、一つのペアのランキングがシーケンス内の別のペアのランキングと一致しているかどうかを検討します。これには、各ペアのデータポイントを他のすべてのペアと比較するプロセスが含まれており、これはケンドールのタウが一致ペアと不一致ペアのみを見るペアワイズ比較よりも包括的です。ヘフディングのDの本質は、データポイントの共同ランキングの評価にあります。これは、比較から導き出された特定の項の合計を計算し、すべてのペアと四つ組を通じた一致度と不一致度の程度を反映します。これらの項は、一つのシーケンス内のデータポイントが、両方のシーケンスを横断して比較したときに、別のポイントよりも上位と下位の両方にランクされる回数を、タイの場合は調整して考慮します。

ヘフディングのDの最終計算には、この合計を正規化する式が含まれており、独立性の仮定の下でのデータポイントの総数と期待値を考慮に入れます。その結果は、-0.5から1までの範囲の尺度で、数値が高いほど、二つのシーケンスが互いに強く依存していることを意味します。お分かりの通り、ある長さの二つのシーケンス（例えば、各シーケンスに5,000個の数字）に対してヘフディングのDを計算することは、個々の比較と計算の膨大な数を含んでおり、ケンドールのタウを導き出すのに使用されるものはおろか、スピアマンのローやピアソンの相関を得るためにも、個々のペアだけでなく、シーケンス全体を考慮に入れ、さらに個々のペアのレベルまで詳細に分析しています。

二つの列がそれぞれ5,000個の数字を含んでいる場合、ヘフディングのDは単に各点を他のすべての点と一度だけ比較するだけではありません（それだけでもかなりの量です）；それは結合されたデータセットからの全ての可能な4つ組の点間の関係を調べます。これを視点に置くと、5,000個の数字の単一の列内のすべての点のペアを比較する場合、約1250万回の比較を行うことになります（5,000から2を選ぶと約1250万になります）。しかし、ヘフディングのDは4つ組を比較することを要求します。5,000の列の中でユニークな4つ組の数は、組み合わせ公式「nから4を選ぶ」によって与えられ、5,000に対しては約62億の4つ組になります。そして、これらの4つ組のそれぞれに対して、ヘフディングのDは、データセット全体の文脈内での一致と不一致を評価するために、複数の比較と計算を伴います。

比較の指数関数的な増加は、ヘフディングのDが著しく計算要求が高い理由を強調しています。これは単にスケールの問題ではなく、ヘフディングのDが二つの列間の複雑な依存関係を捉えるために実行する分析の深さと幅です。この包括的なアプローチにより、ヘフディングのDは、より単純な尺度では見逃されがちな微妙で複雑な関連を検出することができますが、これはまた、特に大規模なデータセットに対しては、その計算がリソースを集中的に使用することを意味します。しかし、私は、安価で高速なコンピューティングの時代になった今、ヘフディングのDの多くの利点を活用し始める時が来たと主張します。二つの列間の任意の種類の関係を、それらの分布に関する仮定をせずに見つけることを可能にするだけでなく、外れ値に対しても頑健であるという利点に加えて、ヘフディングのDにはいくつかの他の利点もあります：それは対称であり、hoeffd(X, Y)はhoeffd(Y, X)と同じであり、常に境界があります（結果は-0.5未満になることも1.0を超えることもありません）、そして一方の列が定数の場合、ヘフディングのDはゼロになります。これは、*相互情報*などの他の強力な関連性の尺度には当てはまらないことです（ここでは議論していません）。

基本的な概要を説明したところで、実際にこれがどのように行われるのか、詳細な部分に入っていきましょう。まず、単一の式を用いて言葉で説明します（式を避けようとすると、すでにある程度複雑で混乱しやすいものがさらに分かりにくく、複雑になる可能性があるので、ここはその一箇所です！）。これが非常に混乱し、ほとんど意味がないように思えるかもしれませんが心配しないでください。通常、アイデアがどのように提示されるかを見ることは有用です（信じてください、元の1948年の論文はさらに解釈が難しいです！）。その後、各部分の直感を得るために、一つ一つ細かく分解していきます。最後に、PythonでのヘフディングのDの実際の（しかし遅い）実装を見ていきます。これには詳細なコメントが付けられています。

**ステップバイステップの分解：**

1. **ランキングとペアワイズ比較**:
    - 最初に、両方の列の各データポイントは、それぞれの列内でのランクに置き換えられます。同じ値には平均ランクが割り当てられ、タイを考慮に入れます。
    - 比較では、これらのランク付けされた列内の全ての可能なペアを見て、それらが一致しているか（両方のランクが共に上がるか下がる）、それとも不一致か（一方のランクは上がり、もう一方は下がる）を決定します。

2. **四重比較**:
    - ペアワイズ比較を超えて、ヘフディングのDはデータポイントの全ての可能な四重組を評価します。各四重組に対して、一組のランク内の順序が別のペアの順序と一致しているかどうかを測定します。このステップは、単純なペアワイズ関連を超えたより複雑な依存関係を捉えるために重要です。これらのランクは**Q**という配列に格納され、各データポイントについて、ランクの一致と不一致の点で他のポイントとの関係を反映する値を持ちます。各データポイントに対して、**Q**は一致（一致する）または不一致（不一致する）なランキング行動を示すペア（別のデータポイントと考えると四重組）の数を累積します。このステップは、ペアワイズ比較では見逃されがちな複雑な依存関係を捉えるために不可欠です。

3. **合計**:
    - ヘフディングのD計算の核心は、全ての四重組にわたる一致と不一致の評価から導かれる特定の項目を合計することに関与します。この合計は、ランクの観測された共同分布が、列が独立である場合に期待されるものからどの程度逸脱しているかを反映します。

4. **正規化**:
    - 最後の計算は、前のステップで得られた合計を正規化することに関わります。この正規化は、データポイントの総数（**N**）を考慮に入れ、独立性の仮定の下での期待値を調整します。この正規化のポイントは、ヘフディングのD統計を異なるサンプルサイズや分布間で比較可能にスケールすることです。
    - 正規化の式は：
    ```math
     D = \frac{30 \times ((N-2)(N-3)D_1 + D_2 - 2(N-2)D_3)}{N(N-1)(N-2)(N-3)(N-4)}
    ```
    ここで、**D_1**、**D_2**、そして**D_3**は、ランクとそれらの一致/不一致の評価の組み合わせに関わる中間の合計です。具体的には：
    - **D_1**は、全ての四つ組のランクの差の積の合計に関連しており、全体的な一致/不一致を反映しています。
    - **D_2**は、各列内の個々の変動性を調整します。
    - **D_3**は、二つの列間の相互作用を考慮に入れます。

それでもまだかなり複雑に見えるので、四つ組や**Q**、そして**D_1**、**D_2**、**D_3**の要素について、もっと詳しく説明しましょう。

**Qとその目的とは?**

- **Q**は、各データポイントに対する重み付けされたカウントを表し、両方の次元（例えば、身長と体重）で自身よりも低いランクを持つ他のポイントがどれだけあるかを考慮し、タイ（同位）に対する調整を加えます。
- データポイント間の一致度と不一致度の程度を捉える上で、単純なペアワイズ比較を超えることが重要です。
- **Q**の計算はタイに対する調整を取り入れており、タイのランクが全体の尺度に適切に寄与するようにします。
- ある点に関して他のすべての点を考慮し、タイに対する調整を取り入れることで、**Q**の値は、ランクの共同分布内での各点の位置を微妙に見せるものであり、これは二つの列間の依存性を評価するための基礎となります。

**D_1, D_2, D_3の解説**:

これらの中間合計は、ヘフディングのDを計算する際に異なる役割を果たします：

  - **D_1**は、独立性の下で期待される量に調整された、すべての4つ組のデータポイント間の一致/不一致を集約したものを反映します。これは、実際のデータがシーケンス間に関係がなかった場合に期待されるものからどれだけ逸脱しているかの尺度です。
      - **D_1に対する直感**：**D_1**は、ランダムな偶然が生み出す以上に、二つのシーケンス間での協調した変動の程度を定量化するものと考えてください。それはランク順序の相互影響の程度を効果的に捉え、ペアの観測が一貫したパターンでより頻繁に一緒に動くかどうかを評価します。
      - **D_1に対する非常にシンプルな直感**：さまざまなパフォーマンスを通じて、二人のダンサー間のダンスムーブの同期を比較していると想像してみてください。**D_1**は、彼らが独立して踊っていた場合に単なる偶然によって期待される以上に、彼らの動きが同期している（または非同期である）程度を表します。それは彼らのパートナーシップの本質を捉え、協調した（または非協調の）努力を定量化することによっています。
  
  - **D_2**は、各シーケンス内のランクの分散の積を表し、シーケンス間の関係に依存しない変動性の基準尺度を提供します。これは、シーケンスの内部変動性とその相互依存性の効果を分離するのに役立ちます。
      - **D_2に対する直感**：**D_2**は、それぞれのシーケンス内の固有の変動性や分散を評価します。これは、各シーケンスでデータポイントがどれだけ広がっているかを理解するために、それぞれのシーケンスが独立してどれだけ変動するかを評価することに似ています。これにより、シーケンス自体の固有の変動性から生じる依存性と、その相互作用から生じる依存性を区別することができます。
      - **D_2に対する非常にシンプルな直感**：**D_2**は、パートナーとは独立して、各ダンサーがパフォーマンスを通じて見せるダンススタイルの範囲を評価すると考えてみてください。それは、各ダンサーがそのパフォーマンスでどれだけ多様または一貫しているかを測ります。この個別の変動性を理解することによって、彼らの同期（**D_1**によって捉えられる）が彼らの相互作用によるものか、それとも個々の傾向によるものかをより良く識別するこ

とができます。
  
  - **D_3**は、**D_1**からの洞察と、**D_2**によって捉えられたシーケンスの内部変動性を組み合わせた相互作用項として機能します。これは、個々のランクの一致が全体の依存性にどのように寄与するかを考慮し、各シーケンスの内部ランク構造を考慮することによって、尺度を微調整します。
      - **D_3に対する直感**：**D_3**は、各シーケンス内の個別の変動性（**D_2**によって捉えられる）が観測された一致/不一致（**D_1**）にどのように影響するかを考慮することによって、尺度を調整します。これは、各シーケンスの内部構造がその共同行動にどのように影響するかを理解することについてであり、個々の変動性の役割を考慮することによって、その関係の評価を洗練します。
      - **D_3に対する非常にシンプルな直感**：**D_3**は、各ダンサーの個々のスタイルと一貫性（**D_2**）が共同パフォーマンス（**D_1**）にどのように影響するかを検討するようなものです。ダンサーが幅広いスタイルを持っている場合、この多様性はパートナーとの同期にどのように影響するでしょうか？**D_3**は、各ダンサーの変動性が協調した努力にどのように影響するかを評価し、彼らのパートナーシップの洗練された視点を提供します。
         
**ヘフディングのDの最終計算**:

- ヘフディングのDの最終公式は、**D_1**、**D_2**、**D_3**を組み合わせ、正規化因子とともに、-0.5から1の範囲の統計量を生成します。
    - **最終計算の直感的理解**: 最終的なヘフディングのD値は、観測された一致/不一致、各列内の固有の変動性、そしてこれらの要因間の相互作用を統合した包括的な尺度です。正規化により、サンプルサイズに適切にスケーリングされた尺度となり、列間の関連の強さと方向の堅牢な指標となります。この尺度は、複雑な相互依存性と個々の変動性を単一の指標に凝縮し、関係の存在だけでなく、その性質や強さも、独立性の下で期待されるものと比較して、全体的な関係を反映します。
    - **非常にシンプルな最終計算の直感**: 最終的なヘフディングのDスコアは、二人のダンサー間の全体的な調和を評価するダンスコンペティションの最終スコアに似ています。この最終スコアは、彼らのパフォーマンスのすべての側面—個々のスタイル、一貫性、お互いの相互作用—を、彼らのダンスパートナーシップの解釈可能な尺度に凝縮します。

ヘフディングのDの計算方法については、今のところかなり良い理解を持っているかもしれませんが、まだ抽象的に思えることでしょうし、実際に実用的なプログラミング言語で全体をどのように実装するかは必ずしも明確ではありません。そこで、今回はPythonで具体的に行ってみます。手短に済ませるために、NumpyとScipyライブラリを利用します：Numpyは配列を効果的に扱うため（便利なインデックス付けを含む）、Scipyは「rankdata」関数を持っており、これはランクを効率的に計算し、平均を使って必要な方法でタイを扱います。

実際のデータポイントを10点だけ使って、物事を本当に具体的にします。これらは(`height_in_inches`, `weight_in_pounds`)の形のデータのペアになります：

```python
X = np.array([55, 62, 68, 70, 72, 65, 67, 78, 78, 78])  # インチ単位の身長
Y = np.array([125, 145, 160, 156, 190, 150, 165, 250, 250, 250])  # ポンド単位の体重
```

この入力でプログラムを実行した結果は以下の通りです：

```python
身長（X）のランク: [1. 2. 5. 6. 7. 3. 4. 9. 9. 9.]
体重（Y）のランク: [1. 2. 5. 4. 7. 3. 6. 9. 9. 9.]
Q値: [1.  2.  4.  4.  7.  3.  4.  8.5 8.5 8.5]
データのヘフディングのD: 0.4107142857142857
```

このセクションでは、具体的なPythonコードを用いてヘフディングのDを計算するプロセスを紹介しています。NumpyとScipyのライブラリを使うことで、配列の効率的な扱いやランクの計算を行い、具体的なデータセットに対してヘフディングのDを求める方法を示しています。

信じられないかもしれませんが、実際にヘフディングのDをPythonで約15行で実装することができます！しかし、できるだけ分かりやすくすることに焦点を当てるため、多くのコメントを追加し、またいくつかの中間値を表示する予定です。その結果、空白を含めて約75行になる予定ですが、コードが行っていることを考えると、それほど多くはありません！

実際にコードを実行したい場合は、こちらのGoogle Colabのノートブックをチェックしてみてください：

https://colab.research.google.com/drive/1jE1h5ENLsf0fhSJ1mj9rJt0gQN3ijqhh?usp=sharing

こちらが完全なコードです：

```python
    import numpy as np
    import scipy.stats
    from scipy.stats import rankdata
            
    def hoeffd_example():
        # 10個のデータポイントで新しいデータセットを作成します。これは（身長（インチ）、体重（ポンド））のペアを表し、最後の3つは同位です。
        X = np.array([55, 62, 68, 70, 72, 65, 67, 78, 78, 78])  # 身長（インチ）
        Y = np.array([125, 145, 160, 156, 190, 150, 165, 250, 250, 250])  # 体重（ポンド）
        # 'average'ランキングメソッドは、同位の値に割り当てられたランクの平均を割り当てます。
        R = rankdata(X, method='average')  # 身長のランク
        S = rankdata(Y, method='average')  # 体重のランク
        # ランクを視覚化するために表示します。最後の3つのエントリーで同位が明らかです。
        print(f"身長のランク (X): {R}")
        print(f"体重のランク (Y): {S}")
        N = len(X)  # データポイントの総数
        # Qは、ヘフディングのD計算に不可欠な特別な合計を各データポイントに対して保持する配列です。
        Q = np.zeros(N)
        # 各データポイントのQ値を計算するためにループします。
        for i in range(N):
            # 各データポイント'i'に対して、より低い身長と体重のランクを持つポイントの数を数えます（一致するペア）。
            Q[i] = 1 + sum(np.logical_and(R < R[i], S < S[i]))
            
            # Q[i]のための同位の調整：両方のランクが等しい場合、部分的に（1/4）Q[i]値に貢献します。
            # "- 1"は、ポイント自体を自分自身との比較に含めないためです。
            Q[i] += (1/4) * (sum(np.logical_and(R == R[i], S == S[i])) - 1)
            
            # 身長のランクが同位でも体重のランクが低い場合、半分（1/2）がQ[i]値に貢献します。
            Q[i] += (1/2) * sum(np.logical_and(R == R[i], S < S[i]))
            
            # 同様に、体重のランクが同位でも身長のランクが低い場合、それも半分（1/2）を貢献します。
            Q[i] += (1/2) * sum(np.logical_and(R < R[i], S == S[i]))
        # 各データポイントのQ値を表示します。「低い」または「等しい」と見なされる点の加重数を示します。
        print(f"Q値: {Q}")

        # ヘフディングのDの計算に必要な中間和を計算します。これらの中間和は、ヘフディングのDの式で必要とされます。
        # D1: この和は、以前に計算されたQ値を活用します。
        # 各Q値は、データポイントのランクが他の両方のシーケンスの中でどのように関連しているかについての情報を包含しています。
        # これには一致（concordance）やタイの調整が含まれます。各データポイントについての項(Q - 1) * (Q - 2)は、このポイン
        # トのランクが他とどの程度一致しているかを、独立性の下で期待される一致に調整して定量化します。これらの項を全データポ
        # イントにわたって合計する（D1）ことで、データセット全体の一致情報を集約します。
        D1 = sum((Q - 1) * (Q - 2))
        
        # D2: この和は、タイに対する調整を加えた各シーケンスのランク差の積に関わります。
        # 各データポイントに対する項(R - 1) * (R - 2) * (S - 1) * (S - 2)は、各シーケンス内のランク分散の相互作用を捉え、
        # ランクの変動性だけに基づいて、独立性の下で期待されるものからどの程度共同ランク分布が逸脱しているかを測定する指標を提供します。
        # これらの積を全データポイントにわたって合計する（D2）ことで、この逸脱の全体的な評価を行います。
        D2 = sum((R - 1) * (R - 2) * (S - 1) * (S - 2))

        # D3：この和は、Q値から得られた洞察とランクの差を組み合わせる相互作用項を表します。各データポイントに対する項
        # （R - 2）（S - 2）（Q - 1）は、ランクの分散とQ値を考慮し、個々のデータポイントのランクの一致／不一致が全体の依存度測定に
        # どのように寄与するかを捉え、独立性の下での期待値で調整します。これらの項（D3）を合計することで、データセットのための包括的
        # な相互作用項にこれらの個別の貢献を統合します。
        D3 = sum((R - 2) * (S - 2) * (Q - 1))

        # ヘフディングのDの最終計算では、D1、D2、およびD3が統合され、サンプルサイズ（N）を考慮した正規化係数が用いられます。
        # この正規化により、ヘフディングのDは適切にスケーリングされ、異なるサイズのデータセット間で意味のある比較が可能になります。
        # この式は、一致、不一致、およびランクの分散の寄与をバランス良く組み込む方法で、これらの合計と正規化係数を取り入れています。
        # その結果、二つの列間の関連の度合いを堅牢に測定する統計量が得られます。
        D = 30 * ((N - 2) * (N - 3) * D1 + D2 - 2 * (N - 2) * D3) / (N * (N - 1) * (N - 2) * (N - 3) * (N - 4))
        # 計算されたヘフディングのD値を返します。
        return D
    # データセットに対してヘフディングのDを計算し、表示します。
    hoeffd_d = hoeffd_example()
    print(f"データに対するヘフディングのD: {hoeffd_d}")
```

さて、この実装は正しいものの、非常に遅く効率が悪いと述べました。これは、実際には非常に効率的なコンパイル済みC++で実装されているNumpyやScipyを使用して「重労働」を行っているにもかかわらず、驚くかもしれません。巨大な数の操作と比較が関わるため、関数のオーバーヘッドが非常に大きく、データポイントが1,000を超えると、速いマシンでもコードが著しく遅くなり始め、5,000データポイントを処理するには極めて長い時間がかかることが判明しました。

幸いなことに、私はRustでずっと効率的なバージョンを書きました。これはPythonライブラリとして簡単に使用できます。以下のようにして使用できます：

`pip install fast_vector_similarity`

コードはこちらで見ることができます：

https://github.com/Dicklesworthstone/fast_vector_similarity

そして、このライブラリについてのHN上での議論はこちらです：

https://news.ycombinator.com/item?id=37234887